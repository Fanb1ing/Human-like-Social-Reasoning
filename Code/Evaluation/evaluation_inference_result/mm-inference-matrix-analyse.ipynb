{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ade0fe2",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "folder_path = 'Path/to/inference'\n",
    "folder_name = \"inference_folder\"\n",
    "\n",
    "\n",
    "file_paths = glob.glob(os.path.join(folder_path, folder_name, \"result_index_*.csv\"))\n",
    "\n",
    "def extract_index(path):\n",
    "    match = re.search(r\"result_index_(\\d+)_\", os.path.basename(path))\n",
    "    return int(match.group(1)) if match else 1e9   \n",
    "file_paths = sorted(file_paths, key=extract_index)\n",
    "\n",
    "merged_df = pd.concat([pd.read_csv(fp) for fp in file_paths], ignore_index=True)\n",
    "print(merged_df.columns) \n",
    "profile_columns = ['Review_age', 'Review_education',\n",
    "       'Review_gender', 'Review_income', 'Review_political',\n",
    "       'Review_religious']\n",
    "res_columns = 'llm_answer'\n",
    "rea_columns = 'llm_reason'\n",
    "user_columns = 'user_choice'\n",
    "merged_df.to_csv(folder_path + folder_name + \"/merged_result.csv\", index=False)\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6354524c",
   "metadata": {},
   "source": [
    "## 提取回答中的答案并转化为0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_text_result(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    filt_str = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9=()（）]', '', text)\n",
    "\n",
    "    match = re.search(r'选择(.*?)$', filt_str)  \n",
    "    if match:\n",
    "        choice = match.group(1)\n",
    "    else:\n",
    "     \n",
    "        choice = ''\n",
    "    return str(choice)\n",
    "\n",
    "merged_df = pd.read_csv(folder_path + folder_name + \"/merged_result.csv\")\n",
    "new_col_name = 'llm_choice_num'\n",
    "merged_df[new_col_name]= merged_df[res_columns].apply(process_text_result)\n",
    "print((merged_df[new_col_name] == '').sum())\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_binary(text):\n",
    "    text = str(text)\n",
    "    if \"no\" in text:\n",
    "        return 0\n",
    "    elif \"yes\" in text and \"no\" not in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "merged_df['user_choice_num'] = merged_df[user_columns].apply(text_to_binary)\n",
    "merged_df[new_col_name] = merged_df[new_col_name].apply(text_to_binary)\n",
    "print(merged_df[new_col_name].isna().sum())\n",
    "merged_df.to_csv(folder_path + folder_name + \"/merged_result_num.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4c528",
   "metadata": {},
   "source": [
    "## cal realism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f08f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def calculate_acc(series,human_series):\n",
    "    correct_predictions = ( series == human_series).sum()\n",
    "    accuracy = correct_predictions / len(series)\n",
    "    return round(accuracy,3)\n",
    "    \n",
    "\n",
    "def calculate_rmse(series,human_series):\n",
    "\n",
    "    if len(series) != len(human_series):\n",
    "        raise ValueError(\"两个序列的长度必须相同\")\n",
    "\n",
    "    series = series.fillna(1 - human_series)\n",
    "\n",
    "    squared_diff = (series - human_series) ** 2\n",
    "    rmse = np.sqrt(np.mean(squared_diff))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def Cal_enr_kl_group(df,groups_column,non_human,human_df):\n",
    "   \n",
    "    choice_columns = ['llm_choice_num','user_choice_num']\n",
    "    result_dict = {} \n",
    "    \n",
    "    df_cleaned = df.dropna(subset=[choice_columns[0]])  # 删除 df 中该列为 NaN 的行\n",
    "    df_cleaned, human_df_cleaned = df_cleaned.align(human_df, join='inner')\n",
    "\n",
    "    temp_kl = calculate_acc(df[choice_columns[0]],human_df[choice_columns[1]])\n",
    "    result_dict['acc'] = temp_kl\n",
    "    print(temp_kl*df_cleaned.shape[0]/df.shape[0])\n",
    "    temp_kl = calculate_rmse(df[choice_columns[0]],human_df[choice_columns[1]])\n",
    "    result_dict['rmse'] = temp_kl\n",
    "\n",
    "    \n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_en_kl(model_name,result_dict,groups_column,save_path,non_human):#,name):\n",
    "    with open(save_path+'/mm_result.json', \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result_dict, f, ensure_ascii=False, indent=2)\n",
    "    print(f'model：{model_name}\\n all-acc:{result_dict[\"acc\"]:.3f}\\nall-rmse:{result_dict[\"rmse\"]:.3f}')\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc21762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def load_data(result_path):\n",
    "    \n",
    "    profile_columns = ['Review_age', 'Review_education',\n",
    "       'Review_gender', 'Review_income', 'Review_political',\n",
    "       'Review_religious'] \n",
    "    df = pd.read_csv(result_path+'/merged_result_num.csv')\n",
    "    for col in profile_columns:\n",
    "        df[col] = df[col].apply(lambda x: -1 if pd.isna(x) or x == 'default' else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "profile_columns = ['Review_age', 'Review_education',\n",
    "       'Review_gender', 'Review_income', 'Review_political',\n",
    "       'Review_religious']\n",
    "choice_columns = ['llm_choice_num','user_choice_num']\n",
    "\n",
    "\n",
    "OLD_FOLDER_PATH= '/data5/fanbingbing/Behave-Benchmark-RL/Result/Step23_MM/0925tempmmcal/'\n",
    "OLD_BASE_MODEL_NAME= ['RRE-SFT-v1s2','RRE-SFT-v1s1']\n",
    "\n",
    "\n",
    "for old_base_model in zip(OLD_BASE_MODEL_NAME):\n",
    "    base_result_path = OLD_FOLDER_PATH+old_base_model\n",
    "    base_df = load_data(base_result_path)\n",
    "    direct_result = Cal_enr_kl_group(base_df,True,base_df) \n",
    "    result_csv = save_en_kl(old_base_model,direct_result,base_result_path,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
