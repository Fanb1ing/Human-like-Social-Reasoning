{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.5,
  "eval_steps": 500,
  "global_step": 325,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007692307692307693,
      "grad_norm": 1.9641469717025757,
      "learning_rate": 0.0,
      "loss": 3.256,
      "step": 1
    },
    {
      "epoch": 0.015384615384615385,
      "grad_norm": 2.296536445617676,
      "learning_rate": 3.0303030303030305e-07,
      "loss": 3.4848,
      "step": 2
    },
    {
      "epoch": 0.023076923076923078,
      "grad_norm": 2.3577895164489746,
      "learning_rate": 6.060606060606061e-07,
      "loss": 3.2771,
      "step": 3
    },
    {
      "epoch": 0.03076923076923077,
      "grad_norm": 2.407716989517212,
      "learning_rate": 9.090909090909091e-07,
      "loss": 3.4469,
      "step": 4
    },
    {
      "epoch": 0.038461538461538464,
      "grad_norm": 1.9246069192886353,
      "learning_rate": 1.2121212121212122e-06,
      "loss": 3.2695,
      "step": 5
    },
    {
      "epoch": 0.046153846153846156,
      "grad_norm": 1.6687440872192383,
      "learning_rate": 1.5151515151515152e-06,
      "loss": 3.0285,
      "step": 6
    },
    {
      "epoch": 0.05384615384615385,
      "grad_norm": 2.2631618976593018,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 3.888,
      "step": 7
    },
    {
      "epoch": 0.06153846153846154,
      "grad_norm": 2.4970507621765137,
      "learning_rate": 2.1212121212121216e-06,
      "loss": 3.654,
      "step": 8
    },
    {
      "epoch": 0.06923076923076923,
      "grad_norm": 2.1123039722442627,
      "learning_rate": 2.4242424242424244e-06,
      "loss": 3.7441,
      "step": 9
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 2.264679193496704,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 3.4955,
      "step": 10
    },
    {
      "epoch": 0.08461538461538462,
      "grad_norm": 2.171445846557617,
      "learning_rate": 3.0303030303030305e-06,
      "loss": 3.7245,
      "step": 11
    },
    {
      "epoch": 0.09230769230769231,
      "grad_norm": 2.0514767169952393,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 3.744,
      "step": 12
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.0338475704193115,
      "learning_rate": 3.6363636363636366e-06,
      "loss": 3.7088,
      "step": 13
    },
    {
      "epoch": 0.1076923076923077,
      "grad_norm": 2.019436836242676,
      "learning_rate": 3.93939393939394e-06,
      "loss": 3.2578,
      "step": 14
    },
    {
      "epoch": 0.11538461538461539,
      "grad_norm": 2.1711061000823975,
      "learning_rate": 4.242424242424243e-06,
      "loss": 3.5299,
      "step": 15
    },
    {
      "epoch": 0.12307692307692308,
      "grad_norm": 2.2866337299346924,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 3.5185,
      "step": 16
    },
    {
      "epoch": 0.13076923076923078,
      "grad_norm": 2.2861530780792236,
      "learning_rate": 4.848484848484849e-06,
      "loss": 3.8361,
      "step": 17
    },
    {
      "epoch": 0.13846153846153847,
      "grad_norm": 1.8575700521469116,
      "learning_rate": 5.151515151515152e-06,
      "loss": 3.6436,
      "step": 18
    },
    {
      "epoch": 0.14615384615384616,
      "grad_norm": 2.376511573791504,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 3.3086,
      "step": 19
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 2.3470849990844727,
      "learning_rate": 5.7575757575757586e-06,
      "loss": 4.0238,
      "step": 20
    },
    {
      "epoch": 0.16153846153846155,
      "grad_norm": 2.71992826461792,
      "learning_rate": 6.060606060606061e-06,
      "loss": 4.074,
      "step": 21
    },
    {
      "epoch": 0.16923076923076924,
      "grad_norm": 2.2314651012420654,
      "learning_rate": 6.363636363636364e-06,
      "loss": 3.5654,
      "step": 22
    },
    {
      "epoch": 0.17692307692307693,
      "grad_norm": 1.8923859596252441,
      "learning_rate": 6.666666666666667e-06,
      "loss": 3.615,
      "step": 23
    },
    {
      "epoch": 0.18461538461538463,
      "grad_norm": 2.284830093383789,
      "learning_rate": 6.969696969696971e-06,
      "loss": 3.1647,
      "step": 24
    },
    {
      "epoch": 0.19230769230769232,
      "grad_norm": 2.2034800052642822,
      "learning_rate": 7.272727272727273e-06,
      "loss": 3.6479,
      "step": 25
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.356391191482544,
      "learning_rate": 7.5757575757575764e-06,
      "loss": 3.5178,
      "step": 26
    },
    {
      "epoch": 0.2076923076923077,
      "grad_norm": 2.4300625324249268,
      "learning_rate": 7.87878787878788e-06,
      "loss": 3.7572,
      "step": 27
    },
    {
      "epoch": 0.2153846153846154,
      "grad_norm": 2.451394557952881,
      "learning_rate": 8.181818181818183e-06,
      "loss": 3.7016,
      "step": 28
    },
    {
      "epoch": 0.2230769230769231,
      "grad_norm": 2.5632474422454834,
      "learning_rate": 8.484848484848486e-06,
      "loss": 3.545,
      "step": 29
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 2.434389352798462,
      "learning_rate": 8.787878787878788e-06,
      "loss": 3.4838,
      "step": 30
    },
    {
      "epoch": 0.23846153846153847,
      "grad_norm": 2.438603162765503,
      "learning_rate": 9.090909090909091e-06,
      "loss": 3.543,
      "step": 31
    },
    {
      "epoch": 0.24615384615384617,
      "grad_norm": 2.525499105453491,
      "learning_rate": 9.393939393939396e-06,
      "loss": 3.4763,
      "step": 32
    },
    {
      "epoch": 0.25384615384615383,
      "grad_norm": 2.585461139678955,
      "learning_rate": 9.696969696969698e-06,
      "loss": 3.6954,
      "step": 33
    },
    {
      "epoch": 0.26153846153846155,
      "grad_norm": 2.5818498134613037,
      "learning_rate": 1e-05,
      "loss": 3.4039,
      "step": 34
    },
    {
      "epoch": 0.2692307692307692,
      "grad_norm": 2.457904100418091,
      "learning_rate": 9.999710619100732e-06,
      "loss": 3.0526,
      "step": 35
    },
    {
      "epoch": 0.27692307692307694,
      "grad_norm": 1.914546012878418,
      "learning_rate": 9.998842509899456e-06,
      "loss": 2.9218,
      "step": 36
    },
    {
      "epoch": 0.2846153846153846,
      "grad_norm": 2.396470546722412,
      "learning_rate": 9.997395772881853e-06,
      "loss": 3.4837,
      "step": 37
    },
    {
      "epoch": 0.2923076923076923,
      "grad_norm": 3.028992176055908,
      "learning_rate": 9.995370575511151e-06,
      "loss": 3.8806,
      "step": 38
    },
    {
      "epoch": 0.3,
      "grad_norm": 2.6357929706573486,
      "learning_rate": 9.992767152208724e-06,
      "loss": 3.8536,
      "step": 39
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 2.5322153568267822,
      "learning_rate": 9.989585804326963e-06,
      "loss": 3.4519,
      "step": 40
    },
    {
      "epoch": 0.3153846153846154,
      "grad_norm": 2.7593252658843994,
      "learning_rate": 9.985826900114391e-06,
      "loss": 3.1529,
      "step": 41
    },
    {
      "epoch": 0.3230769230769231,
      "grad_norm": 2.718852996826172,
      "learning_rate": 9.98149087467304e-06,
      "loss": 3.5036,
      "step": 42
    },
    {
      "epoch": 0.33076923076923076,
      "grad_norm": 2.8759605884552,
      "learning_rate": 9.97657822990809e-06,
      "loss": 3.3394,
      "step": 43
    },
    {
      "epoch": 0.3384615384615385,
      "grad_norm": 3.209439277648926,
      "learning_rate": 9.97108953446976e-06,
      "loss": 3.5511,
      "step": 44
    },
    {
      "epoch": 0.34615384615384615,
      "grad_norm": 2.2018988132476807,
      "learning_rate": 9.965025423687505e-06,
      "loss": 2.9274,
      "step": 45
    },
    {
      "epoch": 0.35384615384615387,
      "grad_norm": 2.9002130031585693,
      "learning_rate": 9.95838659949645e-06,
      "loss": 3.1995,
      "step": 46
    },
    {
      "epoch": 0.36153846153846153,
      "grad_norm": 2.901405096054077,
      "learning_rate": 9.951173830356168e-06,
      "loss": 3.5982,
      "step": 47
    },
    {
      "epoch": 0.36923076923076925,
      "grad_norm": 3.2419593334198,
      "learning_rate": 9.943387951161702e-06,
      "loss": 3.5297,
      "step": 48
    },
    {
      "epoch": 0.3769230769230769,
      "grad_norm": 2.7907283306121826,
      "learning_rate": 9.935029863146946e-06,
      "loss": 3.1812,
      "step": 49
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 2.748478651046753,
      "learning_rate": 9.926100533780304e-06,
      "loss": 3.0961,
      "step": 50
    },
    {
      "epoch": 0.3923076923076923,
      "grad_norm": 2.749842405319214,
      "learning_rate": 9.916600996652726e-06,
      "loss": 2.9397,
      "step": 51
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.6580557823181152,
      "learning_rate": 9.906532351358047e-06,
      "loss": 3.1885,
      "step": 52
    },
    {
      "epoch": 0.4076923076923077,
      "grad_norm": 2.717823028564453,
      "learning_rate": 9.895895763365722e-06,
      "loss": 3.445,
      "step": 53
    },
    {
      "epoch": 0.4153846153846154,
      "grad_norm": 2.4177803993225098,
      "learning_rate": 9.88469246388591e-06,
      "loss": 3.1043,
      "step": 54
    },
    {
      "epoch": 0.4230769230769231,
      "grad_norm": 2.6071460247039795,
      "learning_rate": 9.872923749726959e-06,
      "loss": 2.9387,
      "step": 55
    },
    {
      "epoch": 0.4307692307692308,
      "grad_norm": 3.059279680252075,
      "learning_rate": 9.860590983145307e-06,
      "loss": 3.2616,
      "step": 56
    },
    {
      "epoch": 0.43846153846153846,
      "grad_norm": 2.395296335220337,
      "learning_rate": 9.847695591687788e-06,
      "loss": 2.8944,
      "step": 57
    },
    {
      "epoch": 0.4461538461538462,
      "grad_norm": 3.072235107421875,
      "learning_rate": 9.834239068026388e-06,
      "loss": 3.0618,
      "step": 58
    },
    {
      "epoch": 0.45384615384615384,
      "grad_norm": 3.0810647010803223,
      "learning_rate": 9.82022296978548e-06,
      "loss": 3.1853,
      "step": 59
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 2.517642021179199,
      "learning_rate": 9.805648919361505e-06,
      "loss": 2.7512,
      "step": 60
    },
    {
      "epoch": 0.46923076923076923,
      "grad_norm": 2.468092679977417,
      "learning_rate": 9.790518603735191e-06,
      "loss": 3.0212,
      "step": 61
    },
    {
      "epoch": 0.47692307692307695,
      "grad_norm": 2.4698739051818848,
      "learning_rate": 9.774833774276278e-06,
      "loss": 2.905,
      "step": 62
    },
    {
      "epoch": 0.4846153846153846,
      "grad_norm": 2.467151641845703,
      "learning_rate": 9.758596246540782e-06,
      "loss": 3.0298,
      "step": 63
    },
    {
      "epoch": 0.49230769230769234,
      "grad_norm": 2.2491047382354736,
      "learning_rate": 9.741807900060858e-06,
      "loss": 2.445,
      "step": 64
    },
    {
      "epoch": 0.5,
      "grad_norm": 2.569312810897827,
      "learning_rate": 9.724470678127226e-06,
      "loss": 2.7448,
      "step": 65
    },
    {
      "epoch": 0.5076923076923077,
      "grad_norm": 2.501596689224243,
      "learning_rate": 9.706586587564236e-06,
      "loss": 2.6137,
      "step": 66
    },
    {
      "epoch": 0.5153846153846153,
      "grad_norm": 2.582784414291382,
      "learning_rate": 9.68815769849757e-06,
      "loss": 2.9988,
      "step": 67
    },
    {
      "epoch": 0.5230769230769231,
      "grad_norm": 2.5465943813323975,
      "learning_rate": 9.669186144114627e-06,
      "loss": 3.1892,
      "step": 68
    },
    {
      "epoch": 0.5307692307692308,
      "grad_norm": 2.404773473739624,
      "learning_rate": 9.649674120417591e-06,
      "loss": 3.0241,
      "step": 69
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 2.580909013748169,
      "learning_rate": 9.62962388596925e-06,
      "loss": 2.6262,
      "step": 70
    },
    {
      "epoch": 0.5461538461538461,
      "grad_norm": 2.611175298690796,
      "learning_rate": 9.609037761631552e-06,
      "loss": 2.5303,
      "step": 71
    },
    {
      "epoch": 0.5538461538461539,
      "grad_norm": 2.1536030769348145,
      "learning_rate": 9.587918130296969e-06,
      "loss": 2.9899,
      "step": 72
    },
    {
      "epoch": 0.5615384615384615,
      "grad_norm": 2.06860089302063,
      "learning_rate": 9.566267436612662e-06,
      "loss": 2.3993,
      "step": 73
    },
    {
      "epoch": 0.5692307692307692,
      "grad_norm": 2.175227165222168,
      "learning_rate": 9.544088186697515e-06,
      "loss": 2.43,
      "step": 74
    },
    {
      "epoch": 0.5769230769230769,
      "grad_norm": 1.9265236854553223,
      "learning_rate": 9.521382947852042e-06,
      "loss": 2.3702,
      "step": 75
    },
    {
      "epoch": 0.5846153846153846,
      "grad_norm": 2.1059226989746094,
      "learning_rate": 9.498154348261217e-06,
      "loss": 2.7613,
      "step": 76
    },
    {
      "epoch": 0.5923076923076923,
      "grad_norm": 1.6933319568634033,
      "learning_rate": 9.474405076690257e-06,
      "loss": 2.4919,
      "step": 77
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.114583969116211,
      "learning_rate": 9.450137882173385e-06,
      "loss": 2.919,
      "step": 78
    },
    {
      "epoch": 0.6076923076923076,
      "grad_norm": 2.3832178115844727,
      "learning_rate": 9.425355573695628e-06,
      "loss": 2.5422,
      "step": 79
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 1.9130325317382812,
      "learning_rate": 9.40006101986768e-06,
      "loss": 2.6482,
      "step": 80
    },
    {
      "epoch": 0.6230769230769231,
      "grad_norm": 1.6762326955795288,
      "learning_rate": 9.374257148593824e-06,
      "loss": 2.5588,
      "step": 81
    },
    {
      "epoch": 0.6307692307692307,
      "grad_norm": 2.0652811527252197,
      "learning_rate": 9.347946946733055e-06,
      "loss": 2.5684,
      "step": 82
    },
    {
      "epoch": 0.6384615384615384,
      "grad_norm": 1.7877477407455444,
      "learning_rate": 9.321133459753322e-06,
      "loss": 2.42,
      "step": 83
    },
    {
      "epoch": 0.6461538461538462,
      "grad_norm": 1.605242371559143,
      "learning_rate": 9.293819791379016e-06,
      "loss": 2.6193,
      "step": 84
    },
    {
      "epoch": 0.6538461538461539,
      "grad_norm": 2.0351176261901855,
      "learning_rate": 9.266009103231702e-06,
      "loss": 2.7713,
      "step": 85
    },
    {
      "epoch": 0.6615384615384615,
      "grad_norm": 2.0576729774475098,
      "learning_rate": 9.237704614464157e-06,
      "loss": 2.5734,
      "step": 86
    },
    {
      "epoch": 0.6692307692307692,
      "grad_norm": 1.861307144165039,
      "learning_rate": 9.208909601387748e-06,
      "loss": 2.6132,
      "step": 87
    },
    {
      "epoch": 0.676923076923077,
      "grad_norm": 1.6234663724899292,
      "learning_rate": 9.179627397093184e-06,
      "loss": 2.2071,
      "step": 88
    },
    {
      "epoch": 0.6846153846153846,
      "grad_norm": 1.6826659440994263,
      "learning_rate": 9.149861391064714e-06,
      "loss": 2.2052,
      "step": 89
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 1.5371087789535522,
      "learning_rate": 9.119615028787771e-06,
      "loss": 2.4287,
      "step": 90
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.5501255989074707,
      "learning_rate": 9.088891811350164e-06,
      "loss": 2.3968,
      "step": 91
    },
    {
      "epoch": 0.7076923076923077,
      "grad_norm": 1.5614221096038818,
      "learning_rate": 9.057695295036806e-06,
      "loss": 2.439,
      "step": 92
    },
    {
      "epoch": 0.7153846153846154,
      "grad_norm": 1.5598357915878296,
      "learning_rate": 9.026029090918076e-06,
      "loss": 2.3127,
      "step": 93
    },
    {
      "epoch": 0.7230769230769231,
      "grad_norm": 1.481634259223938,
      "learning_rate": 8.993896864431825e-06,
      "loss": 2.4049,
      "step": 94
    },
    {
      "epoch": 0.7307692307692307,
      "grad_norm": 1.586867332458496,
      "learning_rate": 8.96130233495909e-06,
      "loss": 2.502,
      "step": 95
    },
    {
      "epoch": 0.7384615384615385,
      "grad_norm": 1.3923823833465576,
      "learning_rate": 8.928249275393572e-06,
      "loss": 2.5626,
      "step": 96
    },
    {
      "epoch": 0.7461538461538462,
      "grad_norm": 1.3513453006744385,
      "learning_rate": 8.894741511704911e-06,
      "loss": 2.5854,
      "step": 97
    },
    {
      "epoch": 0.7538461538461538,
      "grad_norm": 1.4952375888824463,
      "learning_rate": 8.860782922495821e-06,
      "loss": 2.5481,
      "step": 98
    },
    {
      "epoch": 0.7615384615384615,
      "grad_norm": 1.4379069805145264,
      "learning_rate": 8.826377438553138e-06,
      "loss": 2.2705,
      "step": 99
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 1.5581659078598022,
      "learning_rate": 8.791529042392813e-06,
      "loss": 2.4921,
      "step": 100
    },
    {
      "epoch": 0.7769230769230769,
      "grad_norm": 1.177456259727478,
      "learning_rate": 8.756241767798934e-06,
      "loss": 2.7058,
      "step": 101
    },
    {
      "epoch": 0.7846153846153846,
      "grad_norm": 1.4782754182815552,
      "learning_rate": 8.720519699356804e-06,
      "loss": 2.5751,
      "step": 102
    },
    {
      "epoch": 0.7923076923076923,
      "grad_norm": 1.7144030332565308,
      "learning_rate": 8.684366971980139e-06,
      "loss": 2.1872,
      "step": 103
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.5810050964355469,
      "learning_rate": 8.647787770432439e-06,
      "loss": 2.2841,
      "step": 104
    },
    {
      "epoch": 0.8076923076923077,
      "grad_norm": 1.48664391040802,
      "learning_rate": 8.610786328842602e-06,
      "loss": 2.0901,
      "step": 105
    },
    {
      "epoch": 0.8153846153846154,
      "grad_norm": 1.685386061668396,
      "learning_rate": 8.573366930214807e-06,
      "loss": 2.4117,
      "step": 106
    },
    {
      "epoch": 0.823076923076923,
      "grad_norm": 1.5420056581497192,
      "learning_rate": 8.535533905932739e-06,
      "loss": 2.6851,
      "step": 107
    },
    {
      "epoch": 0.8307692307692308,
      "grad_norm": 1.2919731140136719,
      "learning_rate": 8.497291635258235e-06,
      "loss": 2.3763,
      "step": 108
    },
    {
      "epoch": 0.8384615384615385,
      "grad_norm": 1.5547767877578735,
      "learning_rate": 8.458644544824371e-06,
      "loss": 2.1843,
      "step": 109
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 1.5426145792007446,
      "learning_rate": 8.419597108123054e-06,
      "loss": 2.4195,
      "step": 110
    },
    {
      "epoch": 0.8538461538461538,
      "grad_norm": 1.3883275985717773,
      "learning_rate": 8.380153844987225e-06,
      "loss": 2.1021,
      "step": 111
    },
    {
      "epoch": 0.8615384615384616,
      "grad_norm": 1.4581494331359863,
      "learning_rate": 8.340319321067668e-06,
      "loss": 2.4416,
      "step": 112
    },
    {
      "epoch": 0.8692307692307693,
      "grad_norm": 1.4389965534210205,
      "learning_rate": 8.300098147304523e-06,
      "loss": 2.6787,
      "step": 113
    },
    {
      "epoch": 0.8769230769230769,
      "grad_norm": 1.3883029222488403,
      "learning_rate": 8.259494979393563e-06,
      "loss": 2.0824,
      "step": 114
    },
    {
      "epoch": 0.8846153846153846,
      "grad_norm": 1.483865737915039,
      "learning_rate": 8.218514517247287e-06,
      "loss": 2.0673,
      "step": 115
    },
    {
      "epoch": 0.8923076923076924,
      "grad_norm": 1.511600136756897,
      "learning_rate": 8.177161504450887e-06,
      "loss": 2.3854,
      "step": 116
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.6398248672485352,
      "learning_rate": 8.135440727713179e-06,
      "loss": 2.336,
      "step": 117
    },
    {
      "epoch": 0.9076923076923077,
      "grad_norm": 1.3666255474090576,
      "learning_rate": 8.093357016312518e-06,
      "loss": 2.183,
      "step": 118
    },
    {
      "epoch": 0.9153846153846154,
      "grad_norm": 1.532119631767273,
      "learning_rate": 8.050915241537802e-06,
      "loss": 2.1851,
      "step": 119
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 1.2014132738113403,
      "learning_rate": 8.008120316124612e-06,
      "loss": 1.9742,
      "step": 120
    },
    {
      "epoch": 0.9307692307692308,
      "grad_norm": 1.669816017150879,
      "learning_rate": 7.964977193686551e-06,
      "loss": 2.3621,
      "step": 121
    },
    {
      "epoch": 0.9384615384615385,
      "grad_norm": 1.6720110177993774,
      "learning_rate": 7.921490868141843e-06,
      "loss": 2.5751,
      "step": 122
    },
    {
      "epoch": 0.9461538461538461,
      "grad_norm": 1.2774465084075928,
      "learning_rate": 7.877666373135287e-06,
      "loss": 2.22,
      "step": 123
    },
    {
      "epoch": 0.9538461538461539,
      "grad_norm": 1.4865443706512451,
      "learning_rate": 7.83350878145559e-06,
      "loss": 2.2236,
      "step": 124
    },
    {
      "epoch": 0.9615384615384616,
      "grad_norm": 1.3727424144744873,
      "learning_rate": 7.789023204448189e-06,
      "loss": 2.1138,
      "step": 125
    },
    {
      "epoch": 0.9692307692307692,
      "grad_norm": 1.3913909196853638,
      "learning_rate": 7.744214791423597e-06,
      "loss": 1.9214,
      "step": 126
    },
    {
      "epoch": 0.9769230769230769,
      "grad_norm": 1.3323503732681274,
      "learning_rate": 7.699088729061355e-06,
      "loss": 2.35,
      "step": 127
    },
    {
      "epoch": 0.9846153846153847,
      "grad_norm": 1.41251802444458,
      "learning_rate": 7.653650240809667e-06,
      "loss": 2.1909,
      "step": 128
    },
    {
      "epoch": 0.9923076923076923,
      "grad_norm": 1.268802285194397,
      "learning_rate": 7.60790458628077e-06,
      "loss": 2.3965,
      "step": 129
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3996528387069702,
      "learning_rate": 7.56185706064212e-06,
      "loss": 2.1283,
      "step": 130
    },
    {
      "epoch": 1.0076923076923077,
      "grad_norm": 1.4680206775665283,
      "learning_rate": 7.5155129940034675e-06,
      "loss": 2.291,
      "step": 131
    },
    {
      "epoch": 1.0153846153846153,
      "grad_norm": 1.5206068754196167,
      "learning_rate": 7.468877750799887e-06,
      "loss": 2.0188,
      "step": 132
    },
    {
      "epoch": 1.023076923076923,
      "grad_norm": 1.348332405090332,
      "learning_rate": 7.421956729170823e-06,
      "loss": 2.11,
      "step": 133
    },
    {
      "epoch": 1.0307692307692307,
      "grad_norm": 1.6846468448638916,
      "learning_rate": 7.374755360335253e-06,
      "loss": 2.5738,
      "step": 134
    },
    {
      "epoch": 1.0384615384615385,
      "grad_norm": 1.4931367635726929,
      "learning_rate": 7.327279107962995e-06,
      "loss": 2.0252,
      "step": 135
    },
    {
      "epoch": 1.0461538461538462,
      "grad_norm": 1.4762012958526611,
      "learning_rate": 7.279533467542295e-06,
      "loss": 2.2015,
      "step": 136
    },
    {
      "epoch": 1.0538461538461539,
      "grad_norm": 1.1193686723709106,
      "learning_rate": 7.2315239657436955e-06,
      "loss": 2.6338,
      "step": 137
    },
    {
      "epoch": 1.0615384615384615,
      "grad_norm": 1.24210786819458,
      "learning_rate": 7.183256159780321e-06,
      "loss": 2.5408,
      "step": 138
    },
    {
      "epoch": 1.0692307692307692,
      "grad_norm": 1.390511155128479,
      "learning_rate": 7.134735636764606e-06,
      "loss": 2.5564,
      "step": 139
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 1.4236842393875122,
      "learning_rate": 7.085968013061585e-06,
      "loss": 2.2638,
      "step": 140
    },
    {
      "epoch": 1.0846153846153845,
      "grad_norm": 1.5642518997192383,
      "learning_rate": 7.036958933638779e-06,
      "loss": 2.4201,
      "step": 141
    },
    {
      "epoch": 1.0923076923076924,
      "grad_norm": 1.470424771308899,
      "learning_rate": 6.987714071412781e-06,
      "loss": 2.4727,
      "step": 142
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.526398777961731,
      "learning_rate": 6.938239126592592e-06,
      "loss": 2.3206,
      "step": 143
    },
    {
      "epoch": 1.1076923076923078,
      "grad_norm": 1.4086415767669678,
      "learning_rate": 6.888539826019824e-06,
      "loss": 2.1942,
      "step": 144
    },
    {
      "epoch": 1.1153846153846154,
      "grad_norm": 1.4292588233947754,
      "learning_rate": 6.8386219225057945e-06,
      "loss": 2.3389,
      "step": 145
    },
    {
      "epoch": 1.123076923076923,
      "grad_norm": 1.51077401638031,
      "learning_rate": 6.788491194165629e-06,
      "loss": 2.2154,
      "step": 146
    },
    {
      "epoch": 1.1307692307692307,
      "grad_norm": 1.2750053405761719,
      "learning_rate": 6.738153443749421e-06,
      "loss": 2.5392,
      "step": 147
    },
    {
      "epoch": 1.1384615384615384,
      "grad_norm": 1.6639033555984497,
      "learning_rate": 6.687614497970567e-06,
      "loss": 2.6255,
      "step": 148
    },
    {
      "epoch": 1.146153846153846,
      "grad_norm": 1.396494746208191,
      "learning_rate": 6.636880206831298e-06,
      "loss": 2.2455,
      "step": 149
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 1.3222814798355103,
      "learning_rate": 6.585956442945531e-06,
      "loss": 2.2264,
      "step": 150
    },
    {
      "epoch": 1.1615384615384616,
      "grad_norm": 1.2989490032196045,
      "learning_rate": 6.534849100859101e-06,
      "loss": 2.4201,
      "step": 151
    },
    {
      "epoch": 1.1692307692307693,
      "grad_norm": 1.6100702285766602,
      "learning_rate": 6.483564096367452e-06,
      "loss": 2.4077,
      "step": 152
    },
    {
      "epoch": 1.176923076923077,
      "grad_norm": 1.270216703414917,
      "learning_rate": 6.432107365830872e-06,
      "loss": 1.7759,
      "step": 153
    },
    {
      "epoch": 1.1846153846153846,
      "grad_norm": 1.3430627584457397,
      "learning_rate": 6.380484865487346e-06,
      "loss": 2.4011,
      "step": 154
    },
    {
      "epoch": 1.1923076923076923,
      "grad_norm": 1.495208501815796,
      "learning_rate": 6.328702570763098e-06,
      "loss": 2.6541,
      "step": 155
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.3641319274902344,
      "learning_rate": 6.276766475580935e-06,
      "loss": 2.2436,
      "step": 156
    },
    {
      "epoch": 1.2076923076923076,
      "grad_norm": 1.3670645952224731,
      "learning_rate": 6.224682591666431e-06,
      "loss": 2.279,
      "step": 157
    },
    {
      "epoch": 1.2153846153846155,
      "grad_norm": 1.3641647100448608,
      "learning_rate": 6.1724569478520495e-06,
      "loss": 2.3026,
      "step": 158
    },
    {
      "epoch": 1.2230769230769232,
      "grad_norm": 1.369099736213684,
      "learning_rate": 6.120095589379299e-06,
      "loss": 2.3516,
      "step": 159
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 1.1707000732421875,
      "learning_rate": 6.067604577198981e-06,
      "loss": 2.3516,
      "step": 160
    },
    {
      "epoch": 1.2384615384615385,
      "grad_norm": 1.3734791278839111,
      "learning_rate": 6.014989987269617e-06,
      "loss": 2.3762,
      "step": 161
    },
    {
      "epoch": 1.2461538461538462,
      "grad_norm": 1.2408334016799927,
      "learning_rate": 5.96225790985415e-06,
      "loss": 2.2057,
      "step": 162
    },
    {
      "epoch": 1.2538461538461538,
      "grad_norm": 1.4627575874328613,
      "learning_rate": 5.909414448814971e-06,
      "loss": 2.2569,
      "step": 163
    },
    {
      "epoch": 1.2615384615384615,
      "grad_norm": 1.584489107131958,
      "learning_rate": 5.856465720907388e-06,
      "loss": 2.359,
      "step": 164
    },
    {
      "epoch": 1.2692307692307692,
      "grad_norm": 1.4856799840927124,
      "learning_rate": 5.803417855071603e-06,
      "loss": 2.3585,
      "step": 165
    },
    {
      "epoch": 1.2769230769230768,
      "grad_norm": 1.4134612083435059,
      "learning_rate": 5.7502769917232635e-06,
      "loss": 2.4887,
      "step": 166
    },
    {
      "epoch": 1.2846153846153845,
      "grad_norm": 1.551387906074524,
      "learning_rate": 5.6970492820426994e-06,
      "loss": 2.4327,
      "step": 167
    },
    {
      "epoch": 1.2923076923076924,
      "grad_norm": 1.4079539775848389,
      "learning_rate": 5.643740887262905e-06,
      "loss": 2.0424,
      "step": 168
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.4765475988388062,
      "learning_rate": 5.59035797795637e-06,
      "loss": 2.3637,
      "step": 169
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 1.4737486839294434,
      "learning_rate": 5.536906733320816e-06,
      "loss": 2.1095,
      "step": 170
    },
    {
      "epoch": 1.3153846153846154,
      "grad_norm": 1.2340948581695557,
      "learning_rate": 5.483393340463938e-06,
      "loss": 2.1696,
      "step": 171
    },
    {
      "epoch": 1.323076923076923,
      "grad_norm": 1.4130710363388062,
      "learning_rate": 5.429823993687234e-06,
      "loss": 2.555,
      "step": 172
    },
    {
      "epoch": 1.3307692307692307,
      "grad_norm": 1.3195124864578247,
      "learning_rate": 5.376204893769e-06,
      "loss": 2.5861,
      "step": 173
    },
    {
      "epoch": 1.3384615384615386,
      "grad_norm": 1.5574499368667603,
      "learning_rate": 5.322542247246583e-06,
      "loss": 2.3963,
      "step": 174
    },
    {
      "epoch": 1.3461538461538463,
      "grad_norm": 1.4792847633361816,
      "learning_rate": 5.26884226569794e-06,
      "loss": 2.6328,
      "step": 175
    },
    {
      "epoch": 1.353846153846154,
      "grad_norm": 1.2344765663146973,
      "learning_rate": 5.215111165022653e-06,
      "loss": 1.9463,
      "step": 176
    },
    {
      "epoch": 1.3615384615384616,
      "grad_norm": 1.3700710535049438,
      "learning_rate": 5.161355164722416e-06,
      "loss": 2.0077,
      "step": 177
    },
    {
      "epoch": 1.3692307692307693,
      "grad_norm": 1.363107442855835,
      "learning_rate": 5.107580487181112e-06,
      "loss": 2.312,
      "step": 178
    },
    {
      "epoch": 1.376923076923077,
      "grad_norm": 1.3701566457748413,
      "learning_rate": 5.0537933569445585e-06,
      "loss": 2.4466,
      "step": 179
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 1.2453715801239014,
      "learning_rate": 5e-06,
      "loss": 2.1464,
      "step": 180
    },
    {
      "epoch": 1.3923076923076922,
      "grad_norm": 1.128071665763855,
      "learning_rate": 4.946206643055443e-06,
      "loss": 2.2848,
      "step": 181
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.3923985958099365,
      "learning_rate": 4.89241951281889e-06,
      "loss": 2.2439,
      "step": 182
    },
    {
      "epoch": 1.4076923076923076,
      "grad_norm": 1.4761261940002441,
      "learning_rate": 4.838644835277585e-06,
      "loss": 2.1426,
      "step": 183
    },
    {
      "epoch": 1.4153846153846155,
      "grad_norm": 1.5196117162704468,
      "learning_rate": 4.784888834977347e-06,
      "loss": 2.104,
      "step": 184
    },
    {
      "epoch": 1.4230769230769231,
      "grad_norm": 1.376548409461975,
      "learning_rate": 4.731157734302063e-06,
      "loss": 2.4149,
      "step": 185
    },
    {
      "epoch": 1.4307692307692308,
      "grad_norm": 1.6749430894851685,
      "learning_rate": 4.6774577527534195e-06,
      "loss": 2.4596,
      "step": 186
    },
    {
      "epoch": 1.4384615384615385,
      "grad_norm": 1.5846710205078125,
      "learning_rate": 4.623795106231001e-06,
      "loss": 2.2981,
      "step": 187
    },
    {
      "epoch": 1.4461538461538461,
      "grad_norm": 1.5516784191131592,
      "learning_rate": 4.570176006312769e-06,
      "loss": 2.342,
      "step": 188
    },
    {
      "epoch": 1.4538461538461538,
      "grad_norm": 1.4471186399459839,
      "learning_rate": 4.516606659536063e-06,
      "loss": 2.4639,
      "step": 189
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 1.3134675025939941,
      "learning_rate": 4.463093266679185e-06,
      "loss": 2.2172,
      "step": 190
    },
    {
      "epoch": 1.4692307692307693,
      "grad_norm": 1.3049348592758179,
      "learning_rate": 4.40964202204363e-06,
      "loss": 1.8346,
      "step": 191
    },
    {
      "epoch": 1.476923076923077,
      "grad_norm": 1.2636953592300415,
      "learning_rate": 4.356259112737096e-06,
      "loss": 2.2971,
      "step": 192
    },
    {
      "epoch": 1.4846153846153847,
      "grad_norm": 1.3325175046920776,
      "learning_rate": 4.302950717957304e-06,
      "loss": 2.1002,
      "step": 193
    },
    {
      "epoch": 1.4923076923076923,
      "grad_norm": 1.1648895740509033,
      "learning_rate": 4.249723008276737e-06,
      "loss": 2.3027,
      "step": 194
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.3854660987854004,
      "learning_rate": 4.196582144928398e-06,
      "loss": 2.2865,
      "step": 195
    },
    {
      "epoch": 1.5076923076923077,
      "grad_norm": 1.6733323335647583,
      "learning_rate": 4.143534279092613e-06,
      "loss": 2.4352,
      "step": 196
    },
    {
      "epoch": 1.5153846153846153,
      "grad_norm": 1.3782612085342407,
      "learning_rate": 4.090585551185031e-06,
      "loss": 1.8417,
      "step": 197
    },
    {
      "epoch": 1.523076923076923,
      "grad_norm": 1.4369182586669922,
      "learning_rate": 4.037742090145851e-06,
      "loss": 2.3535,
      "step": 198
    },
    {
      "epoch": 1.5307692307692307,
      "grad_norm": 1.9369539022445679,
      "learning_rate": 3.985010012730382e-06,
      "loss": 2.0797,
      "step": 199
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 1.325814962387085,
      "learning_rate": 3.93239542280102e-06,
      "loss": 2.4101,
      "step": 200
    },
    {
      "epoch": 1.546153846153846,
      "grad_norm": 1.4830446243286133,
      "learning_rate": 3.879904410620703e-06,
      "loss": 2.2281,
      "step": 201
    },
    {
      "epoch": 1.5538461538461539,
      "grad_norm": 1.6195908784866333,
      "learning_rate": 3.827543052147952e-06,
      "loss": 1.7839,
      "step": 202
    },
    {
      "epoch": 1.5615384615384615,
      "grad_norm": 1.3461248874664307,
      "learning_rate": 3.775317408333571e-06,
      "loss": 2.361,
      "step": 203
    },
    {
      "epoch": 1.5692307692307692,
      "grad_norm": 1.4061893224716187,
      "learning_rate": 3.7232335244190656e-06,
      "loss": 2.1966,
      "step": 204
    },
    {
      "epoch": 1.5769230769230769,
      "grad_norm": 1.4418102502822876,
      "learning_rate": 3.6712974292369035e-06,
      "loss": 2.3412,
      "step": 205
    },
    {
      "epoch": 1.5846153846153848,
      "grad_norm": 1.2259541749954224,
      "learning_rate": 3.6195151345126556e-06,
      "loss": 1.9066,
      "step": 206
    },
    {
      "epoch": 1.5923076923076924,
      "grad_norm": 1.2815124988555908,
      "learning_rate": 3.5678926341691283e-06,
      "loss": 2.2961,
      "step": 207
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.5913909673690796,
      "learning_rate": 3.5164359036325483e-06,
      "loss": 2.1176,
      "step": 208
    },
    {
      "epoch": 1.6076923076923078,
      "grad_norm": 1.4433661699295044,
      "learning_rate": 3.4651508991409016e-06,
      "loss": 2.3933,
      "step": 209
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 1.349165916442871,
      "learning_rate": 3.4140435570544708e-06,
      "loss": 2.1542,
      "step": 210
    },
    {
      "epoch": 1.623076923076923,
      "grad_norm": 1.333836317062378,
      "learning_rate": 3.363119793168704e-06,
      "loss": 2.4085,
      "step": 211
    },
    {
      "epoch": 1.6307692307692307,
      "grad_norm": 1.439965844154358,
      "learning_rate": 3.3123855020294344e-06,
      "loss": 2.5123,
      "step": 212
    },
    {
      "epoch": 1.6384615384615384,
      "grad_norm": 1.2905564308166504,
      "learning_rate": 3.26184655625058e-06,
      "loss": 2.3893,
      "step": 213
    },
    {
      "epoch": 1.646153846153846,
      "grad_norm": 1.2960021495819092,
      "learning_rate": 3.2115088058343725e-06,
      "loss": 2.2949,
      "step": 214
    },
    {
      "epoch": 1.6538461538461537,
      "grad_norm": 1.336717963218689,
      "learning_rate": 3.161378077494205e-06,
      "loss": 2.4044,
      "step": 215
    },
    {
      "epoch": 1.6615384615384614,
      "grad_norm": 1.443261742591858,
      "learning_rate": 3.111460173980175e-06,
      "loss": 2.3081,
      "step": 216
    },
    {
      "epoch": 1.669230769230769,
      "grad_norm": 1.752551794052124,
      "learning_rate": 3.06176087340741e-06,
      "loss": 2.4097,
      "step": 217
    },
    {
      "epoch": 1.676923076923077,
      "grad_norm": 1.381712794303894,
      "learning_rate": 3.0122859285872214e-06,
      "loss": 2.1209,
      "step": 218
    },
    {
      "epoch": 1.6846153846153846,
      "grad_norm": 1.4181032180786133,
      "learning_rate": 2.9630410663612226e-06,
      "loss": 1.9058,
      "step": 219
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 1.5192488431930542,
      "learning_rate": 2.914031986938417e-06,
      "loss": 2.2519,
      "step": 220
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.420844554901123,
      "learning_rate": 2.865264363235396e-06,
      "loss": 2.2301,
      "step": 221
    },
    {
      "epoch": 1.7076923076923078,
      "grad_norm": 1.4246107339859009,
      "learning_rate": 2.816743840219681e-06,
      "loss": 1.9321,
      "step": 222
    },
    {
      "epoch": 1.7153846153846155,
      "grad_norm": 1.3435274362564087,
      "learning_rate": 2.7684760342563045e-06,
      "loss": 2.4373,
      "step": 223
    },
    {
      "epoch": 1.7230769230769232,
      "grad_norm": 1.468059778213501,
      "learning_rate": 2.720466532457707e-06,
      "loss": 2.2871,
      "step": 224
    },
    {
      "epoch": 1.7307692307692308,
      "grad_norm": 1.5842781066894531,
      "learning_rate": 2.6727208920370063e-06,
      "loss": 2.4195,
      "step": 225
    },
    {
      "epoch": 1.7384615384615385,
      "grad_norm": 1.6128283739089966,
      "learning_rate": 2.6252446396647503e-06,
      "loss": 1.9644,
      "step": 226
    },
    {
      "epoch": 1.7461538461538462,
      "grad_norm": 1.5421431064605713,
      "learning_rate": 2.578043270829178e-06,
      "loss": 2.5527,
      "step": 227
    },
    {
      "epoch": 1.7538461538461538,
      "grad_norm": 1.5220428705215454,
      "learning_rate": 2.531122249200114e-06,
      "loss": 2.3237,
      "step": 228
    },
    {
      "epoch": 1.7615384615384615,
      "grad_norm": 1.5473368167877197,
      "learning_rate": 2.4844870059965337e-06,
      "loss": 2.2474,
      "step": 229
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 1.7535728216171265,
      "learning_rate": 2.438142939357882e-06,
      "loss": 2.2555,
      "step": 230
    },
    {
      "epoch": 1.7769230769230768,
      "grad_norm": 1.3697936534881592,
      "learning_rate": 2.392095413719231e-06,
      "loss": 2.0647,
      "step": 231
    },
    {
      "epoch": 1.7846153846153845,
      "grad_norm": 1.3216086626052856,
      "learning_rate": 2.346349759190332e-06,
      "loss": 2.1927,
      "step": 232
    },
    {
      "epoch": 1.7923076923076922,
      "grad_norm": 1.3164118528366089,
      "learning_rate": 2.3009112709386454e-06,
      "loss": 1.9844,
      "step": 233
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.5023928880691528,
      "learning_rate": 2.2557852085764053e-06,
      "loss": 2.2981,
      "step": 234
    },
    {
      "epoch": 1.8076923076923077,
      "grad_norm": 1.189832091331482,
      "learning_rate": 2.2109767955518135e-06,
      "loss": 1.913,
      "step": 235
    },
    {
      "epoch": 1.8153846153846154,
      "grad_norm": 1.5641812086105347,
      "learning_rate": 2.1664912185444127e-06,
      "loss": 2.4977,
      "step": 236
    },
    {
      "epoch": 1.823076923076923,
      "grad_norm": 1.4200903177261353,
      "learning_rate": 2.1223336268647154e-06,
      "loss": 2.2792,
      "step": 237
    },
    {
      "epoch": 1.830769230769231,
      "grad_norm": 1.5522440671920776,
      "learning_rate": 2.0785091318581577e-06,
      "loss": 2.3565,
      "step": 238
    },
    {
      "epoch": 1.8384615384615386,
      "grad_norm": 1.1928669214248657,
      "learning_rate": 2.035022806313449e-06,
      "loss": 1.917,
      "step": 239
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 1.4848406314849854,
      "learning_rate": 1.991879683875386e-06,
      "loss": 2.3812,
      "step": 240
    },
    {
      "epoch": 1.853846153846154,
      "grad_norm": 1.4738095998764038,
      "learning_rate": 1.9490847584621993e-06,
      "loss": 2.3954,
      "step": 241
    },
    {
      "epoch": 1.8615384615384616,
      "grad_norm": 1.4587006568908691,
      "learning_rate": 1.9066429836874844e-06,
      "loss": 2.1576,
      "step": 242
    },
    {
      "epoch": 1.8692307692307693,
      "grad_norm": 1.3970909118652344,
      "learning_rate": 1.8645592722868223e-06,
      "loss": 2.2486,
      "step": 243
    },
    {
      "epoch": 1.876923076923077,
      "grad_norm": 1.278406023979187,
      "learning_rate": 1.8228384955491136e-06,
      "loss": 2.1417,
      "step": 244
    },
    {
      "epoch": 1.8846153846153846,
      "grad_norm": 1.3885669708251953,
      "learning_rate": 1.7814854827527144e-06,
      "loss": 2.4367,
      "step": 245
    },
    {
      "epoch": 1.8923076923076922,
      "grad_norm": 1.4616503715515137,
      "learning_rate": 1.7405050206064372e-06,
      "loss": 2.2334,
      "step": 246
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.4349594116210938,
      "learning_rate": 1.6999018526954775e-06,
      "loss": 2.5882,
      "step": 247
    },
    {
      "epoch": 1.9076923076923076,
      "grad_norm": 1.4197036027908325,
      "learning_rate": 1.6596806789323317e-06,
      "loss": 2.0288,
      "step": 248
    },
    {
      "epoch": 1.9153846153846152,
      "grad_norm": 1.5939204692840576,
      "learning_rate": 1.6198461550127758e-06,
      "loss": 2.4344,
      "step": 249
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 1.3812475204467773,
      "learning_rate": 1.5804028918769488e-06,
      "loss": 2.3271,
      "step": 250
    },
    {
      "epoch": 1.9307692307692308,
      "grad_norm": 1.3704757690429688,
      "learning_rate": 1.5413554551756321e-06,
      "loss": 2.3768,
      "step": 251
    },
    {
      "epoch": 1.9384615384615385,
      "grad_norm": 1.4158673286437988,
      "learning_rate": 1.5027083647417657e-06,
      "loss": 2.3398,
      "step": 252
    },
    {
      "epoch": 1.9461538461538461,
      "grad_norm": 1.6818031072616577,
      "learning_rate": 1.4644660940672628e-06,
      "loss": 2.2426,
      "step": 253
    },
    {
      "epoch": 1.953846153846154,
      "grad_norm": 1.364206314086914,
      "learning_rate": 1.4266330697851955e-06,
      "loss": 2.2744,
      "step": 254
    },
    {
      "epoch": 1.9615384615384617,
      "grad_norm": 1.420469045639038,
      "learning_rate": 1.3892136711573983e-06,
      "loss": 1.8369,
      "step": 255
    },
    {
      "epoch": 1.9692307692307693,
      "grad_norm": 1.3568824529647827,
      "learning_rate": 1.3522122295675616e-06,
      "loss": 2.4637,
      "step": 256
    },
    {
      "epoch": 1.976923076923077,
      "grad_norm": 1.4588861465454102,
      "learning_rate": 1.3156330280198637e-06,
      "loss": 2.418,
      "step": 257
    },
    {
      "epoch": 1.9846153846153847,
      "grad_norm": 1.5950571298599243,
      "learning_rate": 1.2794803006431984e-06,
      "loss": 2.4237,
      "step": 258
    },
    {
      "epoch": 1.9923076923076923,
      "grad_norm": 1.4462859630584717,
      "learning_rate": 1.2437582322010672e-06,
      "loss": 2.5263,
      "step": 259
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.4931282997131348,
      "learning_rate": 1.2084709576071885e-06,
      "loss": 2.4898,
      "step": 260
    },
    {
      "epoch": 2.0076923076923077,
      "grad_norm": 1.3557316064834595,
      "learning_rate": 1.1736225614468627e-06,
      "loss": 2.6297,
      "step": 261
    },
    {
      "epoch": 2.0153846153846153,
      "grad_norm": 1.293055772781372,
      "learning_rate": 1.1392170775041788e-06,
      "loss": 1.7986,
      "step": 262
    },
    {
      "epoch": 2.023076923076923,
      "grad_norm": 1.4743577241897583,
      "learning_rate": 1.1052584882950896e-06,
      "loss": 2.3642,
      "step": 263
    },
    {
      "epoch": 2.0307692307692307,
      "grad_norm": 1.4868601560592651,
      "learning_rate": 1.0717507246064273e-06,
      "loss": 1.9147,
      "step": 264
    },
    {
      "epoch": 2.0384615384615383,
      "grad_norm": 1.318406343460083,
      "learning_rate": 1.0386976650409102e-06,
      "loss": 2.2455,
      "step": 265
    },
    {
      "epoch": 2.046153846153846,
      "grad_norm": 1.4485602378845215,
      "learning_rate": 1.0061031355681766e-06,
      "loss": 2.1545,
      "step": 266
    },
    {
      "epoch": 2.0538461538461537,
      "grad_norm": 1.5323240756988525,
      "learning_rate": 9.739709090819254e-07,
      "loss": 2.0577,
      "step": 267
    },
    {
      "epoch": 2.0615384615384613,
      "grad_norm": 1.255540132522583,
      "learning_rate": 9.423047049631956e-07,
      "loss": 2.2842,
      "step": 268
    },
    {
      "epoch": 2.0692307692307694,
      "grad_norm": 1.3449338674545288,
      "learning_rate": 9.111081886498374e-07,
      "loss": 2.4208,
      "step": 269
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 1.4815235137939453,
      "learning_rate": 8.803849712122292e-07,
      "loss": 2.1045,
      "step": 270
    },
    {
      "epoch": 2.0846153846153848,
      "grad_norm": 1.6075347661972046,
      "learning_rate": 8.501386089352858e-07,
      "loss": 2.4697,
      "step": 271
    },
    {
      "epoch": 2.0923076923076924,
      "grad_norm": 1.3837236166000366,
      "learning_rate": 8.203726029068149e-07,
      "loss": 2.0597,
      "step": 272
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.2933416366577148,
      "learning_rate": 7.910903986122537e-07,
      "loss": 2.7731,
      "step": 273
    },
    {
      "epoch": 2.1076923076923078,
      "grad_norm": 1.5043710470199585,
      "learning_rate": 7.622953855358456e-07,
      "loss": 2.4612,
      "step": 274
    },
    {
      "epoch": 2.1153846153846154,
      "grad_norm": 1.5184301137924194,
      "learning_rate": 7.339908967683007e-07,
      "loss": 2.0209,
      "step": 275
    },
    {
      "epoch": 2.123076923076923,
      "grad_norm": 1.3553961515426636,
      "learning_rate": 7.061802086209857e-07,
      "loss": 2.192,
      "step": 276
    },
    {
      "epoch": 2.1307692307692307,
      "grad_norm": 1.345065951347351,
      "learning_rate": 6.788665402466782e-07,
      "loss": 2.124,
      "step": 277
    },
    {
      "epoch": 2.1384615384615384,
      "grad_norm": 1.5621942281723022,
      "learning_rate": 6.52053053266945e-07,
      "loss": 2.2125,
      "step": 278
    },
    {
      "epoch": 2.146153846153846,
      "grad_norm": 1.459707260131836,
      "learning_rate": 6.257428514061764e-07,
      "loss": 2.3597,
      "step": 279
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 1.502886414527893,
      "learning_rate": 5.999389801323219e-07,
      "loss": 2.2706,
      "step": 280
    },
    {
      "epoch": 2.1615384615384614,
      "grad_norm": 1.4492530822753906,
      "learning_rate": 5.746444263043715e-07,
      "loss": 2.3301,
      "step": 281
    },
    {
      "epoch": 2.169230769230769,
      "grad_norm": 1.3352760076522827,
      "learning_rate": 5.498621178266167e-07,
      "loss": 2.2352,
      "step": 282
    },
    {
      "epoch": 2.1769230769230767,
      "grad_norm": 1.3553180694580078,
      "learning_rate": 5.255949233097451e-07,
      "loss": 2.2126,
      "step": 283
    },
    {
      "epoch": 2.184615384615385,
      "grad_norm": 1.596781611442566,
      "learning_rate": 5.018456517387837e-07,
      "loss": 2.3253,
      "step": 284
    },
    {
      "epoch": 2.1923076923076925,
      "grad_norm": 1.3623833656311035,
      "learning_rate": 4.786170521479588e-07,
      "loss": 2.5893,
      "step": 285
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.4987750053405762,
      "learning_rate": 4.5591181330248534e-07,
      "loss": 2.2171,
      "step": 286
    },
    {
      "epoch": 2.207692307692308,
      "grad_norm": 1.6190052032470703,
      "learning_rate": 4.3373256338733847e-07,
      "loss": 2.1936,
      "step": 287
    },
    {
      "epoch": 2.2153846153846155,
      "grad_norm": 1.5474742650985718,
      "learning_rate": 4.1208186970303097e-07,
      "loss": 2.1776,
      "step": 288
    },
    {
      "epoch": 2.223076923076923,
      "grad_norm": 1.4797018766403198,
      "learning_rate": 3.90962238368448e-07,
      "loss": 2.1724,
      "step": 289
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 1.2461109161376953,
      "learning_rate": 3.70376114030751e-07,
      "loss": 2.2034,
      "step": 290
    },
    {
      "epoch": 2.2384615384615385,
      "grad_norm": 1.5025105476379395,
      "learning_rate": 3.503258795824105e-07,
      "loss": 2.2682,
      "step": 291
    },
    {
      "epoch": 2.246153846153846,
      "grad_norm": 1.39390230178833,
      "learning_rate": 3.308138558853746e-07,
      "loss": 2.2604,
      "step": 292
    },
    {
      "epoch": 2.253846153846154,
      "grad_norm": 1.4819698333740234,
      "learning_rate": 3.1184230150243025e-07,
      "loss": 2.3317,
      "step": 293
    },
    {
      "epoch": 2.2615384615384615,
      "grad_norm": 1.1979349851608276,
      "learning_rate": 2.934134124357646e-07,
      "loss": 2.0375,
      "step": 294
    },
    {
      "epoch": 2.269230769230769,
      "grad_norm": 1.2938348054885864,
      "learning_rate": 2.755293218727739e-07,
      "loss": 2.2601,
      "step": 295
    },
    {
      "epoch": 2.276923076923077,
      "grad_norm": 1.3841079473495483,
      "learning_rate": 2.5819209993914185e-07,
      "loss": 2.3154,
      "step": 296
    },
    {
      "epoch": 2.2846153846153845,
      "grad_norm": 1.1881645917892456,
      "learning_rate": 2.4140375345921895e-07,
      "loss": 2.156,
      "step": 297
    },
    {
      "epoch": 2.292307692307692,
      "grad_norm": 1.646508812904358,
      "learning_rate": 2.2516622572372416e-07,
      "loss": 2.0222,
      "step": 298
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.4488354921340942,
      "learning_rate": 2.094813962648101e-07,
      "loss": 2.1525,
      "step": 299
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 1.486880898475647,
      "learning_rate": 1.9435108063849684e-07,
      "loss": 2.0999,
      "step": 300
    },
    {
      "epoch": 2.315384615384615,
      "grad_norm": 1.6320425271987915,
      "learning_rate": 1.7977703021452185e-07,
      "loss": 2.6926,
      "step": 301
    },
    {
      "epoch": 2.3230769230769233,
      "grad_norm": 1.4119733572006226,
      "learning_rate": 1.6576093197361253e-07,
      "loss": 2.0033,
      "step": 302
    },
    {
      "epoch": 2.330769230769231,
      "grad_norm": 1.28956139087677,
      "learning_rate": 1.523044083122138e-07,
      "loss": 2.4448,
      "step": 303
    },
    {
      "epoch": 2.3384615384615386,
      "grad_norm": 1.3033124208450317,
      "learning_rate": 1.39409016854693e-07,
      "loss": 2.1052,
      "step": 304
    },
    {
      "epoch": 2.3461538461538463,
      "grad_norm": 1.381259560585022,
      "learning_rate": 1.2707625027304104e-07,
      "loss": 2.1119,
      "step": 305
    },
    {
      "epoch": 2.353846153846154,
      "grad_norm": 1.5029383897781372,
      "learning_rate": 1.1530753611409151e-07,
      "loss": 2.3938,
      "step": 306
    },
    {
      "epoch": 2.3615384615384616,
      "grad_norm": 1.3925449848175049,
      "learning_rate": 1.041042366342787e-07,
      "loss": 2.1159,
      "step": 307
    },
    {
      "epoch": 2.3692307692307693,
      "grad_norm": 1.6327170133590698,
      "learning_rate": 9.346764864195335e-08,
      "loss": 2.2457,
      "step": 308
    },
    {
      "epoch": 2.376923076923077,
      "grad_norm": 1.5610549449920654,
      "learning_rate": 8.339900334727536e-08,
      "loss": 2.2916,
      "step": 309
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 1.473960041999817,
      "learning_rate": 7.389946621969679e-08,
      "loss": 2.3439,
      "step": 310
    },
    {
      "epoch": 2.3923076923076922,
      "grad_norm": 1.4600626230239868,
      "learning_rate": 6.497013685305586e-08,
      "loss": 2.0973,
      "step": 311
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.3496290445327759,
      "learning_rate": 5.661204883829763e-08,
      "loss": 2.2637,
      "step": 312
    },
    {
      "epoch": 2.4076923076923076,
      "grad_norm": 1.4838815927505493,
      "learning_rate": 4.8826169643832464e-08,
      "loss": 2.0284,
      "step": 313
    },
    {
      "epoch": 2.4153846153846152,
      "grad_norm": 1.2980444431304932,
      "learning_rate": 4.1613400503550114e-08,
      "loss": 2.3323,
      "step": 314
    },
    {
      "epoch": 2.423076923076923,
      "grad_norm": 1.5787386894226074,
      "learning_rate": 3.4974576312497564e-08,
      "loss": 2.1778,
      "step": 315
    },
    {
      "epoch": 2.430769230769231,
      "grad_norm": 1.3349995613098145,
      "learning_rate": 2.8910465530240793e-08,
      "loss": 2.0857,
      "step": 316
    },
    {
      "epoch": 2.4384615384615387,
      "grad_norm": 1.244687557220459,
      "learning_rate": 2.3421770091912044e-08,
      "loss": 2.1165,
      "step": 317
    },
    {
      "epoch": 2.4461538461538463,
      "grad_norm": 1.5176613330841064,
      "learning_rate": 1.850912532696092e-08,
      "loss": 2.2256,
      "step": 318
    },
    {
      "epoch": 2.453846153846154,
      "grad_norm": 1.3565772771835327,
      "learning_rate": 1.4173099885610997e-08,
      "loss": 2.1944,
      "step": 319
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 1.483932614326477,
      "learning_rate": 1.041419567303914e-08,
      "loss": 2.119,
      "step": 320
    },
    {
      "epoch": 2.4692307692307693,
      "grad_norm": 1.3891018629074097,
      "learning_rate": 7.2328477912769756e-09,
      "loss": 2.1106,
      "step": 321
    },
    {
      "epoch": 2.476923076923077,
      "grad_norm": 1.2562733888626099,
      "learning_rate": 4.629424488850065e-09,
      "loss": 2.4312,
      "step": 322
    },
    {
      "epoch": 2.4846153846153847,
      "grad_norm": 1.5353999137878418,
      "learning_rate": 2.604227118148117e-09,
      "loss": 2.2959,
      "step": 323
    },
    {
      "epoch": 2.4923076923076923,
      "grad_norm": 1.355794906616211,
      "learning_rate": 1.1574901005456662e-09,
      "loss": 2.3586,
      "step": 324
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.3742011785507202,
      "learning_rate": 2.89380899267111e-10,
      "loss": 2.3981,
      "step": 325
    },
    {
      "epoch": 2.5,
      "step": 325,
      "total_flos": 3.4869343479791616e+16,
      "train_loss": 2.5255136841994066,
      "train_runtime": 1382.401,
      "train_samples_per_second": 1.881,
      "train_steps_per_second": 0.235
    }
  ],
  "logging_steps": 1,
  "max_steps": 325,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.4869343479791616e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
